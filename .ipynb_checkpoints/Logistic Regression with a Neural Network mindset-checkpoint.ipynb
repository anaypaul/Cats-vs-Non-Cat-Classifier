{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(set_x):\n",
    "    x = 1 / (1 + np.exp(-set_x))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    w = np.zeros(shape = (dim,1) )\n",
    "    b = 1\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)  # compute activation\n",
    "    cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))  # compute cost\n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dw = (1 / m) * np.dot(X, (A - Y).T)\n",
    "    db = (1 / m) * np.sum(A - Y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    # w= weights of the parameters\n",
    "    # b = bias\n",
    "    # X = the input dataset\n",
    "    # Y = input labels\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    #forward propagation\n",
    "    A = sigmoid(np.dot(w.T,X) + b)\n",
    "    cost = (  -1 / m ) * (np.sum( Y * np.log(A) + (1-Y) *np.log(1-A) ))\n",
    "    \n",
    "    #backward propagation\n",
    "    dw = (1/m) * np.dot(X,(A-Y).T)\n",
    "    db = (1/m) * np.sum(A-Y)\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\":dw,\"db\":db}\n",
    "#     print(grads[\"dw\"], grads[\"db\"])\n",
    "    return grads,cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate * dw  # need to broadcast\n",
    "        b = b - learning_rate * db\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if i % 100 == 0:\n",
    "            \n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        # Convert probabilities a[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 17  31  56]\n",
      "   [ 22  33  59]\n",
      "   [ 25  35  62]\n",
      "   ...\n",
      "   [  1  28  57]\n",
      "   [  1  26  56]\n",
      "   [  1  22  51]]\n",
      "\n",
      "  [[ 25  36  62]\n",
      "   [ 28  38  64]\n",
      "   [ 30  40  67]\n",
      "   ...\n",
      "   [  1  27  56]\n",
      "   [  1  25  55]\n",
      "   [  2  21  51]]\n",
      "\n",
      "  [[ 32  40  67]\n",
      "   [ 34  42  69]\n",
      "   [ 35  42  70]\n",
      "   ...\n",
      "   [  1  25  55]\n",
      "   [  0  24  54]\n",
      "   [  1  21  51]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]]\n",
      "\n",
      "\n",
      " [[[196 192 190]\n",
      "   [193 186 182]\n",
      "   [188 179 174]\n",
      "   ...\n",
      "   [ 90 142 200]\n",
      "   [ 90 142 201]\n",
      "   [ 90 142 201]]\n",
      "\n",
      "  [[230 229 229]\n",
      "   [204 199 197]\n",
      "   [193 186 181]\n",
      "   ...\n",
      "   [ 91 143 201]\n",
      "   [ 91 143 201]\n",
      "   [ 91 143 201]]\n",
      "\n",
      "  [[232 225 224]\n",
      "   [235 234 234]\n",
      "   [208 205 202]\n",
      "   ...\n",
      "   [ 91 144 202]\n",
      "   [ 91 144 202]\n",
      "   [ 92 144 202]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 18  17  15]\n",
      "   [ 14  14  13]\n",
      "   [ 29  29  32]\n",
      "   ...\n",
      "   [ 83  81  81]\n",
      "   [ 84  82  83]\n",
      "   [ 82  81  82]]\n",
      "\n",
      "  [[ 22  20  18]\n",
      "   [ 16  15  14]\n",
      "   [ 25  24  24]\n",
      "   ...\n",
      "   [ 82  80  80]\n",
      "   [ 83  81  82]\n",
      "   [ 82  81  81]]\n",
      "\n",
      "  [[ 45  43  39]\n",
      "   [ 61  59  54]\n",
      "   [ 81  78  74]\n",
      "   ...\n",
      "   [ 83  82  81]\n",
      "   [ 84  82  82]\n",
      "   [ 82  80  81]]]\n",
      "\n",
      "\n",
      " [[[ 82  71  68]\n",
      "   [ 89  83  83]\n",
      "   [100  98 104]\n",
      "   ...\n",
      "   [131 132 137]\n",
      "   [126 124 124]\n",
      "   [105  97  95]]\n",
      "\n",
      "  [[ 95  91  97]\n",
      "   [104 104 113]\n",
      "   [110 115 126]\n",
      "   ...\n",
      "   [135 134 135]\n",
      "   [127 122 119]\n",
      "   [111 105 103]]\n",
      "\n",
      "  [[ 94  85  83]\n",
      "   [ 97  89  90]\n",
      "   [110 109 115]\n",
      "   ...\n",
      "   [136 134 131]\n",
      "   [127 120 117]\n",
      "   [116 108 104]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 96 116 131]\n",
      "   [ 97 115 130]\n",
      "   [103 123 139]\n",
      "   ...\n",
      "   [152 155 157]\n",
      "   [146 149 152]\n",
      "   [130 133 134]]\n",
      "\n",
      "  [[ 90 108 123]\n",
      "   [ 92 108 121]\n",
      "   [100 119 134]\n",
      "   ...\n",
      "   [150 152 155]\n",
      "   [144 146 147]\n",
      "   [134 135 134]]\n",
      "\n",
      "  [[ 86 102 116]\n",
      "   [ 87 103 115]\n",
      "   [ 94 114 127]\n",
      "   ...\n",
      "   [154 156 160]\n",
      "   [146 148 152]\n",
      "   [138 141 142]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[143 155 165]\n",
      "   [184 190 198]\n",
      "   [142 149 155]\n",
      "   ...\n",
      "   [ 99  92 102]\n",
      "   [120  98 102]\n",
      "   [100  84  95]]\n",
      "\n",
      "  [[151 149 139]\n",
      "   [173 179 185]\n",
      "   [105 135 141]\n",
      "   ...\n",
      "   [ 91  87  99]\n",
      "   [119  99 104]\n",
      "   [120  95 101]]\n",
      "\n",
      "  [[204 190 185]\n",
      "   [180 185 195]\n",
      "   [117 155 177]\n",
      "   ...\n",
      "   [ 96  88 101]\n",
      "   [125 103 110]\n",
      "   [120 100 110]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 41  80 116]\n",
      "   [ 41  80 116]\n",
      "   [ 41  78 115]\n",
      "   ...\n",
      "   [ 63  75  98]\n",
      "   [ 60  72  98]\n",
      "   [ 60  70  96]]\n",
      "\n",
      "  [[ 71  90 121]\n",
      "   [ 73  91 123]\n",
      "   [ 74  91 124]\n",
      "   ...\n",
      "   [ 79 101 142]\n",
      "   [ 80 100 140]\n",
      "   [ 82 101 139]]\n",
      "\n",
      "  [[ 71  88 122]\n",
      "   [ 73  92 128]\n",
      "   [ 76  95 131]\n",
      "   ...\n",
      "   [ 81 106 150]\n",
      "   [ 85 108 151]\n",
      "   [ 85 107 149]]]\n",
      "\n",
      "\n",
      " [[[ 22  24  23]\n",
      "   [ 23  25  24]\n",
      "   [ 24  26  25]\n",
      "   ...\n",
      "   [ 24  29  25]\n",
      "   [ 23  25  22]\n",
      "   [ 20  22  21]]\n",
      "\n",
      "  [[ 22  24  23]\n",
      "   [ 23  25  24]\n",
      "   [ 23  26  25]\n",
      "   ...\n",
      "   [ 22  28  23]\n",
      "   [ 20  23  22]\n",
      "   [ 19  21  21]]\n",
      "\n",
      "  [[ 22  24  22]\n",
      "   [ 23  25  24]\n",
      "   [ 23  26  25]\n",
      "   ...\n",
      "   [ 23  27  23]\n",
      "   [ 20  23  21]\n",
      "   [ 18  20  19]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  8   5   0]\n",
      "   [  9   6   1]\n",
      "   [  9   6   1]\n",
      "   ...\n",
      "   [  4   5   0]\n",
      "   [  5   4   0]\n",
      "   [  4   5   0]]\n",
      "\n",
      "  [[  7   5   0]\n",
      "   [  8   5   1]\n",
      "   [  9   6   1]\n",
      "   ...\n",
      "   [  4   5   0]\n",
      "   [  4   5   0]\n",
      "   [  4   5   0]]\n",
      "\n",
      "  [[  7   5   0]\n",
      "   [  8   5   0]\n",
      "   [  9   6   1]\n",
      "   ...\n",
      "   [  4   5   0]\n",
      "   [  4   5   0]\n",
      "   [  4   5   0]]]\n",
      "\n",
      "\n",
      " [[[  8  28  53]\n",
      "   [ 14  33  58]\n",
      "   [ 19  35  61]\n",
      "   ...\n",
      "   [ 11  16  35]\n",
      "   [ 10  16  35]\n",
      "   [  9  14  32]]\n",
      "\n",
      "  [[ 15  31  57]\n",
      "   [ 15  32  58]\n",
      "   [ 18  34  60]\n",
      "   ...\n",
      "   [ 13  17  35]\n",
      "   [ 13  17  35]\n",
      "   [ 13  16  35]]\n",
      "\n",
      "  [[ 20  35  61]\n",
      "   [ 19  33  59]\n",
      "   [ 20  33  59]\n",
      "   ...\n",
      "   [ 16  17  35]\n",
      "   [ 16  18  35]\n",
      "   [ 15  17  35]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]]] (209, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [0], it's a 'non-cat' picture.\n"
     ]
    }
   ],
   "source": [
    "# pictures of the dataset\n",
    "print(train_set_x_orig,train_set_x_orig.shape)\n",
    "for i in range(len(train_set_x_orig)):\n",
    "    index = i\n",
    "    example = train_set_x_orig[index]\n",
    "    plt.imshow(train_set_x_orig[index])\n",
    "    plt.show()\n",
    "    print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no of training samples : 209\n",
      " no of testing samples : 50\n",
      " Height and width of an image in the sample : 64 64 3\n"
     ]
    }
   ],
   "source": [
    "print(\" no of training samples : {0}\".format(train_set_y.shape[1]))\n",
    "print(\" no of testing samples : {0}\".format(test_set_y.shape[1]))\n",
    "print(\" Height and width of an image in the sample : {0} {1} {2}\".format(train_set_x_orig.shape[1],train_set_x_orig.shape[2],train_set_x_orig.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "m_train = train_set_y.shape[1]\n",
    "m_test = test_set_y.shape[1]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 209) (12288, 50)\n",
      "*****sanity checking *****\n",
      "[17 31 56 22 33 59 25 35 62 25 35 62 27 36 64 28 38 67 30 41 69 31 43 73\n",
      " 32 47 76 34 49 79 35 50 82 36 51 82 35 50 81 34 49 79 33 48 79 33 48 79\n",
      " 32 47 78 31 46 76 30 44 75 29 44 75 29 44 75 27 44 74 27 42 73 25 41 71\n",
      " 23 40 72 21 41 73 21 42 74 21 41 74 20 40 73 20 39 72 19 39 72 18 38 71\n",
      " 16 38 70 14 37 69 12 37 68 11 36 67  9 36 66  7 34 64  7 35 66  4 36 69\n",
      "  3 36 69  2 34 65  2 34 65  1 35 67  1 34 67  1 34 66  0 32]\n",
      "[[[[ 17  31  56]\n",
      "   [ 22  33  59]\n",
      "   [ 25  35  62]\n",
      "   ...\n",
      "   [  1  28  57]\n",
      "   [  1  26  56]\n",
      "   [  1  22  51]]\n",
      "\n",
      "  [[ 25  36  62]\n",
      "   [ 28  38  64]\n",
      "   [ 30  40  67]\n",
      "   ...\n",
      "   [  1  27  56]\n",
      "   [  1  25  55]\n",
      "   [  2  21  51]]\n",
      "\n",
      "  [[ 32  40  67]\n",
      "   [ 34  42  69]\n",
      "   [ 35  42  70]\n",
      "   ...\n",
      "   [  1  25  55]\n",
      "   [  0  24  54]\n",
      "   [  1  21  51]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]]\n",
      "\n",
      "\n",
      " [[[196 192 190]\n",
      "   [193 186 182]\n",
      "   [188 179 174]\n",
      "   ...\n",
      "   [ 90 142 200]\n",
      "   [ 90 142 201]\n",
      "   [ 90 142 201]]\n",
      "\n",
      "  [[230 229 229]\n",
      "   [204 199 197]\n",
      "   [193 186 181]\n",
      "   ...\n",
      "   [ 91 143 201]\n",
      "   [ 91 143 201]\n",
      "   [ 91 143 201]]\n",
      "\n",
      "  [[232 225 224]\n",
      "   [235 234 234]\n",
      "   [208 205 202]\n",
      "   ...\n",
      "   [ 91 144 202]\n",
      "   [ 91 144 202]\n",
      "   [ 92 144 202]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 18  17  15]\n",
      "   [ 14  14  13]\n",
      "   [ 29  29  32]\n",
      "   ...\n",
      "   [ 83  81  81]\n",
      "   [ 84  82  83]\n",
      "   [ 82  81  82]]\n",
      "\n",
      "  [[ 22  20  18]\n",
      "   [ 16  15  14]\n",
      "   [ 25  24  24]\n",
      "   ...\n",
      "   [ 82  80  80]\n",
      "   [ 83  81  82]\n",
      "   [ 82  81  81]]\n",
      "\n",
      "  [[ 45  43  39]\n",
      "   [ 61  59  54]\n",
      "   [ 81  78  74]\n",
      "   ...\n",
      "   [ 83  82  81]\n",
      "   [ 84  82  82]\n",
      "   [ 82  80  81]]]\n",
      "\n",
      "\n",
      " [[[ 82  71  68]\n",
      "   [ 89  83  83]\n",
      "   [100  98 104]\n",
      "   ...\n",
      "   [131 132 137]\n",
      "   [126 124 124]\n",
      "   [105  97  95]]\n",
      "\n",
      "  [[ 95  91  97]\n",
      "   [104 104 113]\n",
      "   [110 115 126]\n",
      "   ...\n",
      "   [135 134 135]\n",
      "   [127 122 119]\n",
      "   [111 105 103]]\n",
      "\n",
      "  [[ 94  85  83]\n",
      "   [ 97  89  90]\n",
      "   [110 109 115]\n",
      "   ...\n",
      "   [136 134 131]\n",
      "   [127 120 117]\n",
      "   [116 108 104]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 96 116 131]\n",
      "   [ 97 115 130]\n",
      "   [103 123 139]\n",
      "   ...\n",
      "   [152 155 157]\n",
      "   [146 149 152]\n",
      "   [130 133 134]]\n",
      "\n",
      "  [[ 90 108 123]\n",
      "   [ 92 108 121]\n",
      "   [100 119 134]\n",
      "   ...\n",
      "   [150 152 155]\n",
      "   [144 146 147]\n",
      "   [134 135 134]]\n",
      "\n",
      "  [[ 86 102 116]\n",
      "   [ 87 103 115]\n",
      "   [ 94 114 127]\n",
      "   ...\n",
      "   [154 156 160]\n",
      "   [146 148 152]\n",
      "   [138 141 142]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[143 155 165]\n",
      "   [184 190 198]\n",
      "   [142 149 155]\n",
      "   ...\n",
      "   [ 99  92 102]\n",
      "   [120  98 102]\n",
      "   [100  84  95]]\n",
      "\n",
      "  [[151 149 139]\n",
      "   [173 179 185]\n",
      "   [105 135 141]\n",
      "   ...\n",
      "   [ 91  87  99]\n",
      "   [119  99 104]\n",
      "   [120  95 101]]\n",
      "\n",
      "  [[204 190 185]\n",
      "   [180 185 195]\n",
      "   [117 155 177]\n",
      "   ...\n",
      "   [ 96  88 101]\n",
      "   [125 103 110]\n",
      "   [120 100 110]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 41  80 116]\n",
      "   [ 41  80 116]\n",
      "   [ 41  78 115]\n",
      "   ...\n",
      "   [ 63  75  98]\n",
      "   [ 60  72  98]\n",
      "   [ 60  70  96]]\n",
      "\n",
      "  [[ 71  90 121]\n",
      "   [ 73  91 123]\n",
      "   [ 74  91 124]\n",
      "   ...\n",
      "   [ 79 101 142]\n",
      "   [ 80 100 140]\n",
      "   [ 82 101 139]]\n",
      "\n",
      "  [[ 71  88 122]\n",
      "   [ 73  92 128]\n",
      "   [ 76  95 131]\n",
      "   ...\n",
      "   [ 81 106 150]\n",
      "   [ 85 108 151]\n",
      "   [ 85 107 149]]]\n",
      "\n",
      "\n",
      " [[[ 22  24  23]\n",
      "   [ 23  25  24]\n",
      "   [ 24  26  25]\n",
      "   ...\n",
      "   [ 24  29  25]\n",
      "   [ 23  25  22]\n",
      "   [ 20  22  21]]\n",
      "\n",
      "  [[ 22  24  23]\n",
      "   [ 23  25  24]\n",
      "   [ 23  26  25]\n",
      "   ...\n",
      "   [ 22  28  23]\n",
      "   [ 20  23  22]\n",
      "   [ 19  21  21]]\n",
      "\n",
      "  [[ 22  24  22]\n",
      "   [ 23  25  24]\n",
      "   [ 23  26  25]\n",
      "   ...\n",
      "   [ 23  27  23]\n",
      "   [ 20  23  21]\n",
      "   [ 18  20  19]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  8   5   0]\n",
      "   [  9   6   1]\n",
      "   [  9   6   1]\n",
      "   ...\n",
      "   [  4   5   0]\n",
      "   [  5   4   0]\n",
      "   [  4   5   0]]\n",
      "\n",
      "  [[  7   5   0]\n",
      "   [  8   5   1]\n",
      "   [  9   6   1]\n",
      "   ...\n",
      "   [  4   5   0]\n",
      "   [  4   5   0]\n",
      "   [  4   5   0]]\n",
      "\n",
      "  [[  7   5   0]\n",
      "   [  8   5   0]\n",
      "   [  9   6   1]\n",
      "   ...\n",
      "   [  4   5   0]\n",
      "   [  4   5   0]\n",
      "   [  4   5   0]]]\n",
      "\n",
      "\n",
      " [[[  8  28  53]\n",
      "   [ 14  33  58]\n",
      "   [ 19  35  61]\n",
      "   ...\n",
      "   [ 11  16  35]\n",
      "   [ 10  16  35]\n",
      "   [  9  14  32]]\n",
      "\n",
      "  [[ 15  31  57]\n",
      "   [ 15  32  58]\n",
      "   [ 18  34  60]\n",
      "   ...\n",
      "   [ 13  17  35]\n",
      "   [ 13  17  35]\n",
      "   [ 13  16  35]]\n",
      "\n",
      "  [[ 20  35  61]\n",
      "   [ 19  33  59]\n",
      "   [ 20  33  59]\n",
      "   ...\n",
      "   [ 16  17  35]\n",
      "   [ 16  18  35]\n",
      "   [ 15  17  35]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]]]\n"
     ]
    }
   ],
   "source": [
    "### reshaping the training and test data set in \n",
    "### train_set_x_flatten that contains image samples stored as columns and each row is a features \n",
    "train_set_x_flatten  = train_set_x_orig.reshape(train_set_x_orig.shape[0], train_set_x_orig.shape[1]*train_set_x_orig.shape[2]*train_set_x_orig.shape[3]).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "print(train_set_x_flatten.shape, test_set_x_flatten.shape)\n",
    "print(\"*****sanity checking *****\")\n",
    "print(str(train_set_x_flatten[:140,0]))\n",
    "print(train_set_x_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standarding the dataset by taking the mean and subtracting the data by mean and dividing it with the standard deviation.\n",
    "#but where the range of the data is same, so here we just divide the data by 255\n",
    "train_set_x = train_set_x_flatten / 255\n",
    "test_set_x = test_set_x_flatten / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dw': array([[0.99993216],\n",
      "       [1.99980262]]), 'db': 0.49993523062470574}\n",
      "cost = 6.000064773192205\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print(grads)\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[-0.98731056]\n",
      " [-0.19170416]]\n",
      "b = 1.8978031995333131\n",
      "dw = [[ 0.09232179]\n",
      " [-0.02386335]]\n",
      "db = -0.05809257200047277\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 1000, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
